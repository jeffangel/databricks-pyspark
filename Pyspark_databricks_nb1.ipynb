{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d94e5adb-33ed-44b5-ab85-aa724a0c958e",
     "showTitle": true,
     "title": "Importando Variables"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d8e2bd2-e60a-49a9-b1f6-877c67ba88f4",
     "showTitle": true,
     "title": "Crear una Sesión de Spark"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3e9afef-6703-4046-a76a-db0831fe969e",
     "showTitle": true,
     "title": "Versión de Spark"
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c67432a4-39fe-4d80-94fb-a9453312a0aa",
     "showTitle": true,
     "title": "Terminar la sesión de Spark"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d3ca500-04bc-4da7-a2fe-7e8d88036066",
     "showTitle": true,
     "title": "(RDD) Resilient Distributed Dataset"
    }
   },
   "source": [
    "1. Una colección de elementos tolerantes a fallo.\n",
    "2. Puede operar en paralelo.\n",
    "\n",
    "Caracteristicas principales:\n",
    "1. Tipo de dato básico que utiliza Apache Spark.\n",
    "2. Estan particionados en distintos nodos del cluster. Los RDD's van a estar distribuidos por todas las instancias\n",
    "3. Tolerancia a fallos. Si un fichero se carga, y el nodo falla, este se puede ejecutar en otra instancia.\n",
    "4. Se crea a partir de ficheros en HDFS (Hadoop File System) / DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df7b24d7-634d-47e0-b918-873d151ae6ea",
     "showTitle": true,
     "title": "Creación de un RDD"
    }
   },
   "outputs": [],
   "source": [
    "#creamos una lista\n",
    "\n",
    "lista = ['Apache Spark','Curso de Databricks','Bigdata','Azure Cloud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35c05a75-e971-49b3-859d-8d61a8300d61",
     "showTitle": true,
     "title": "Nota:"
    }
   },
   "source": [
    "1. Para aplicaciones de producción, se crea un RDD principalmente usando sistema de almacenamiento externo como un HDFS, S3 HBase, entre otros.\n",
    "2. Parallelize, es una función en Spark que te permite crear un RDD a partir de una colección de lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3fc7261-1fab-4d04-b1cd-7498ecbe08c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Crear un RDD utilizando parallelize\n",
    "rdd = spark.sparkContext.parallelize(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf715e57-2af1-4192-8607-333f1343e63e",
     "showTitle": true,
     "title": "Imprimir la colección de un RDD"
    }
   },
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbfb749b-c07a-4804-bcc5-b79dcfeadd79",
     "showTitle": true,
     "title": "Nota 2"
    }
   },
   "source": [
    "\n",
    "1. Crear un RDD vació usando sparkContext.emptyRDD, solo para tenerlo instanciado. Este RDD al estar vacío, va sin partición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1167c78-acc2-471c-bb99-8877216a4bcf",
     "showTitle": true,
     "title": "Crear un RDD vacío"
    }
   },
   "outputs": [],
   "source": [
    "rddempty = spark.sparkContext.emptyRDD()\n",
    "rddempty.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9736ce9-1b5d-40cd-b6ce-27fbd544da85",
     "showTitle": true,
     "title": "Leyendo archivo en local"
    }
   },
   "outputs": [],
   "source": [
    "df_spark = spark.read.table(\"ventas_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5178085-360b-4fd3-a455-f7c0704a8ff3",
     "showTitle": true,
     "title": "Transformación de un dataframde a un RDD"
    }
   },
   "outputs": [],
   "source": [
    "rdd2 = df_spark.rdd\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0795857-66c4-49ad-9d44-6723f4484b1f",
     "showTitle": true,
     "title": "Map()"
    }
   },
   "source": [
    "Se utiliza para hacer operaciones más complejas. Por ejemplo: agregar una columna, actualizarla, salida de transformación, contestos, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9008c7b1-b20e-4bef-8cd7-d6ed9fb670fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda x: (x,1))\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49959324-0bea-438b-a11e-9cafabed3b27",
     "showTitle": true,
     "title": "Otras funciones en RDD"
    }
   },
   "source": [
    "1. reduceByKey() --> Fusión de cada clave para formar una cadena. Similar a un concat.\n",
    "2. SortByKey() --> Ordenar, es similar a un order by\n",
    "3. Filter() --> Filtros en un RDD\n",
    "4. First() --> Devuelve el primer registro\n",
    "5. Max() --> El maximo registro\n",
    "6. Reduce() --> Reduce los registros a uno solo, luego con esto puedes contar, sumar, totalizar, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8beebe11-d424-43fe-ae79-c6e13d0255e7",
     "showTitle": true,
     "title": "Convertir un RDD a un Dataframe"
    }
   },
   "outputs": [],
   "source": [
    "#Creamos una lista\n",
    "data = [(\"Scala\",\"5000\"),(\"Python\",\"5000\"),(\"Java\",\"30000\")]\n",
    "\n",
    "#Convirtiendo a rdd\n",
    "rdd6 = spark.sparkContext.parallelize(data)\n",
    "\n",
    "#Agregar columnas al DF desde un RDD\n",
    "columns = [\"lenguaje\",\"usuarios\"]\n",
    "\n",
    "#Convertir de RDD a Dataframe+\n",
    "df_from_rdd6 = rdd6.toDF(columns)\n",
    "\n",
    "df_from_rdd6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6522634-846a-4d26-bf9f-39d88ca38982",
     "showTitle": true,
     "title": "Opción 2"
    }
   },
   "outputs": [],
   "source": [
    "df_from_data = spark.createDataFrame(data).toDF(*columns)\n",
    "df_from_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5d01070-b1fd-42e1-8bd0-0b159cb30f58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Agregar columnas al DF desde un RDD\n",
    "columns = [\"lenguaje\",\"usuarios\"]\n",
    "df_from_rdd6 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70f94fcf-b16e-47c1-831d-40f9c8d6fdb3",
     "showTitle": true,
     "title": "Crear un Dataframe con esquemas"
    }
   },
   "source": [
    "En caso se requiera especificar los nombres de las columnas, así como el tipo de datos, primero se debe crear el esquema StructType, y luego asignarlo mientras se crea el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bb6283b-15e4-4e3d-b3b2-6499cc6e9743",
     "showTitle": true,
     "title": "Importar librerías para StructType"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edc5ba18-1654-4729-8a8b-be4a69c8a028",
     "showTitle": true,
     "title": "Crear una lista"
    }
   },
   "outputs": [],
   "source": [
    "data3 = [(\"Pedro\",\"Alonso\",\"Ibañez\",44545311,\"M\",10000),\n",
    "         (\"Juan\",\"Pedro\",\"Salazar\",44545312,\"M\",20000),\n",
    "         (\"Alberto\",\"Iam\",\"Godines\",44545313,\"M\",30000),\n",
    "         (\"Maria\",\"Miranda\",\"Diaz\",44545314,\"F\",40000),\n",
    "         (\"Raquel\",\"Ali\",\"Guzman\",44545315,\"F\",50000)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab285d49-9b46-46a6-98c6-7d10046b14f1",
     "showTitle": true,
     "title": "Asignar el tipo de dato a la lista"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"PrimerNombre\",StringType(),True),\n",
    "                     StructField(\"SegundoNombre\",StringType(),True),\n",
    "                     StructField(\"Apellido\",StringType(),True),\n",
    "                     StructField(\"DNI\",IntegerType(),True),\n",
    "                     StructField(\"Sexo\",StringType(),True),\n",
    "                     StructField(\"Salario\",IntegerType(),True)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e58b6f73-6785-42d0-810e-36ea60b13f6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Crear el dataframe\n",
    "df = spark.createDataFrame(data=data3,schema=schema)\n",
    "\n",
    "#Imprimir el esquema\n",
    "df.printSchema()\n",
    "\n",
    "#Mostrar los datos del dataframe\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5f623c1-c802-46c3-916c-dc88cb4d4228",
     "showTitle": true,
     "title": "Crear dataframes desde otras fuentes de datos"
    }
   },
   "source": [
    "\n",
    "1. Desde un CSV (indistitamente donde este alojado) --> df_csv = spark.read.csv(\"path/file.csv\")\n",
    "2. Desde un TXT (indistitamente donde este alojado) --> df_text = spark.read.text(\"path/file.txt\")\n",
    "3. Desde un JSON (indistitamente donde este alojado) --> df_json = spark.read.json(\"path/file.json\")\n",
    "4. Otras fuentes (AVRO, PARQUET, KAFKA, Tablas Hive/Base, entre otros) --> spark.read.<tipo_archivo>(\"path/file.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256a6595-9485-4fbf-af96-b79972192c17",
     "showTitle": true,
     "title": "Tarea"
    }
   },
   "outputs": [],
   "source": [
    "#Importando las librerias necesarias\n",
    "from pyspark.sql.types import *\n",
    "#Ruta a cargar\n",
    "path =\"/FileStore/tables/ventas-1.txt\"\n",
    "\n",
    "#Definir un esquema\n",
    "schema = StructType([StructField(\"NCLICOD\",IntegerType(),True), \\\n",
    "                StructField(\"PRDDCOD\",StringType(),True), \\\n",
    "                StructField(\"VTAPIMPVEN\",DecimalType(10,2),True), \\\n",
    "                StructField(\"VTAPIGV\",DecimalType(10,2),True), \\\n",
    "                StructField(\"VTAPMTOTOT\",DecimalType(10,2),True), \\\n",
    "                StructField(\"VTAPFCH\",StringType(),True) \\\n",
    "                    ])\n",
    "\n",
    "df_with_schema = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\",\").schema(schema).load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8920c78c-c9fb-4d56-ad67-8f2d7c358e84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a63dfac-a132-4163-95a5-3b85f16e84b0",
     "showTitle": true,
     "title": "Comentarios Adicionales"
    }
   },
   "outputs": [],
   "source": [
    "StructType y StructField son clases que se usa con pyspark, y van a permitir gestionar las estructuras de datos con el dataframe para que se pueda hacer las conversiones de las mismas, estructuras anidadas, matrices, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4893cf88-bec7-4e41-9575-ecceacbbdae3",
     "showTitle": true,
     "title": "Uso de Select"
    }
   },
   "outputs": [],
   "source": [
    "df_with_schema.select(\"NCLICOD\",\"PRDDCOD\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "829df131-7fd8-4b93-8e49-66a41edb8692",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Opción 2\n",
    "df_with_schema.select(df_with_schema.NCLICOD, df_with_schema.PRDDCOD).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "537eaad9-9451-4c1a-b38e-323789c25143",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Opción 3\n",
    "\n",
    "df_with_schema.select(df_with_schema[\"NCLICOD\"],df_with_schema[\"PRDDCOD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61fdca84-b876-4600-b274-ec6e04c268df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Opción 4: Usando la función COL()\n",
    "from pyspark.sql.functions import col\n",
    "df_with_schema.select(col(\"NCLICOD\"),col(\"PRDDCOD\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66ce5de6-c4d1-4f2d-8026-c32f02d84bbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Seleccionamos campos con nombres de columnas filtradas. Esto es similar a un like % en SQL\n",
    "df_with_schema.select(df_with_schema.colRegex(\"`^.*COD*`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ff70199-dad5-4499-8394-9fa5a5815276",
     "showTitle": true,
     "title": "Uso de WithColumn()"
    }
   },
   "source": [
    "Es una función de transformacuón, que permite cambiar el valor, convertir el tipo de dato o crear una columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd13c325-8524-4770-a5c6-37b2b40f5aa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Castero de variable\n",
    "\n",
    "df_with_schema.withColumn(\"VTAPFCH\",col(\"VTAPFCH\").cast(\"String\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b3d6724-8b53-4c69-a2d4-f69e68db557e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Actualizar el valor del campo\n",
    "df_with_schema.withColumn(\"VTAPIMPVEN\",col(\"VTAPIMPVEN\")*1.2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c298f8b4-5c83-4f67-be24-58db4aa1621e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Crear una nueva columna con lógica\n",
    "df_with_schema.withColumn(\"DEDUCCIONES\",col(\"VTAPIGV\")*0.3).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "873617e6-c164-4e80-8d25-199a0d5b9b42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Renombar una columna\n",
    "df_with_schema.withColumnRenamed(\"NCLICOD\",\"COD_CLIENTE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d87852fd-7cec-4fd0-9c39-66266b33346d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Eliminar una columna\n",
    "df_with_schema.drop(\"NombreColumna\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bcfac8c-5942-4e40-a127-ab6c5aa4ed52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Ejercicio:\n",
    "#Desarrolle usted una reporte donde tenga la siguiente estructura:\n",
    "\n",
    "#CODIGO_CLIENTE\n",
    "#CODIGO_PRODUCTO\n",
    "#VENTA_PARCIAL\n",
    "#VENTA_IGV\n",
    "#VENTA_TOTAL\n",
    "#FECHA_VENTA\n",
    "#DESCUENTO = 0.15*VENTA_TOTAL\n",
    "#DESCUENTO_FINAL = DESCUENTO*0.10\n",
    "#MONTO_CON_DESCUENTOS= VENTA_TOTAL-DESCUENTO-DESCUENTO_FINAL\n",
    "\n",
    "#ORDERNADO DE MAYOR A MENOR MONTO_CON_DESCUENTOS\n",
    "\n",
    "df_with_schema.withColumnRenamed(\"NCLICOD\",\"CODIGO_CLIENTE\")\\\n",
    ".withColumnRenamed(\"PRDDCOD\",\"CODIGO_PRODUCTO\")\\\n",
    ".withColumnRenamed(\"VTAPIMPVEN\",\"VENTA_PARCIAL\")\\\n",
    ".withColumnRenamed(\"VTAPIGV\",\"VENTA_IGV\")\\\n",
    ".withColumnRenamed(\"VTAPMTOTOT\",\"VENTA_TOTAL\")\\\n",
    ".withColumnRenamed(\"VTAPFCH\",\"FECHA_VENTA\")\\\n",
    ".withColumn(\"DESCUENTO\",col(\"VENTA_TOTAL\")*0.15)\\\n",
    ".withColumn(\"DESCUENTO_FINAL\",col(\"DESCUENTO\")*0.1)\\\n",
    ".withColumn(\"MONTO_CON_DESCUENTOS\",col(\"VENTA_TOTAL\")-col(\"DESCUENTO\")-col(\"DESCUENTO_FINAL\"))\\\n",
    ".orderBy(col(\"MONTO_CON_DESCUENTOS\")\\\n",
    ".desc()).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d88d87c-4807-43f3-a4d6-02b0e1d71884",
     "showTitle": true,
     "title": "Filtro por contenido de las celdas"
    }
   },
   "outputs": [],
   "source": [
    "df_with_schema.filter(df_with_schema.NCLICOD == 43793).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d98f437-e232-45a0-b165-521e8fead894",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1.\tCree un proceso, donde una vez que la data se deje en una ruta, este pueda cargarse de forma directa, con los tipos de datos que corresponden  a cada variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ec16399-691c-4af2-bdd7-5cdb130d9ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "866e91c4-c9d3-4852-ba42-16a64059c574",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Importando las librerias necesarias\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f29f0fac-5ba7-4af6-b66a-7ba10d890b9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Ruta a cargar\n",
    "path =\"/FileStore/tables/t2.csv\"\n",
    "\n",
    "#Definir un esquema\n",
    "schema = StructType([StructField(\"\",IntegerType(),True), \\\n",
    "                StructField(\"ID\",IntegerType(),True), \\\n",
    "                StructField(\"NIVEL_EDUCATIVO\",StringType(),True), \\\n",
    "                StructField(\"SEXO\",StringType(),True), \\\n",
    "                StructField(\"CATEGORIA_EMPLEO\",StringType(),True), \\\n",
    "                StructField(\"EXPERIENCIA_LABORAL\",StringType(),True), \\\n",
    "                StructField(\"ESTADO_CIVIL\",StringType(),True), \\\n",
    "                StructField(\"EDAD\",IntegerType(),True), \\\n",
    "                StructField(\"UTILIZACION_TARJETAS\",IntegerType(),True), \\\n",
    "                StructField(\"NUMERO_ENTIDADES\",StringType(),True), \\\n",
    "                StructField(\"DEFAULT\",IntegerType(),True), \\\n",
    "                StructField(\"TARGET\",IntegerType(),True), \\\n",
    "                StructField(\"NUM_ENT_W\",DecimalType(10,2),True), \\\n",
    "                StructField(\"EDUC_W\",DecimalType(10,2),True), \\\n",
    "                StructField(\"EXP_LAB_W\",DecimalType(10,2),True), \\\n",
    "                StructField(\"EDAD_W\",DecimalType(10,2),True), \\\n",
    "                StructField(\"UTIL_TC_W\",DecimalType(10,2),True), \\\n",
    "                StructField(\"PD\",DoubleType(),True), \\\n",
    "                StructField(\"RPD\",DoubleType(),True)\n",
    "                    ])\n",
    "\n",
    "#df_with_schema=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\",\").schema(schema).load(path)\n",
    "df_with_schema = spark.read.option(\"header\", \"true\").option(\"ignoreLeadingWhiteSpace\",\"true\").schema(schema).csv(path)\n",
    "df_with_schema=df_with_schema.drop(\"\")\n",
    "df_with_schema.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1948bd06-c606-401f-a200-15e847a6b974",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.\tElimine todas las variables que contienen peso (terminan en _W) y liste el dataframe depurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c738fbaf-60a3-423f-809e-027767007c3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "temp = df_with_schema.select(df_with_schema.colRegex(\"`^.*_W`\"))\n",
    "df_with_schema = df_with_schema.drop(*temp.columns)\n",
    "df_with_schema.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54ad443c-28c3-464f-811e-9ebae306605d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.\tListe un reporte donde muestre solo las personas con nivel educativo “Estudios Universitarios” con una “PD” más alta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3539ff3-40e9-4a0b-a995-11bf5090ac1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, col\n",
    "display(df_with_schema.filter(df_with_schema.NIVEL_EDUCATIVO=='Estudios Universitarios')\\\n",
    "        .select(\"ID\",\"PD\")\\\n",
    "        .orderBy(desc(\"PD\")).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86651095-94d4-4bb8-9c02-4947607a88f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.\tTomando la pregunta 3 de referencia, indique solo los primeros 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d71d219-0b94-4be7-9fa9-32084d026bfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_with_schema.filter(df_with_schema.NIVEL_EDUCATIVO=='Estudios Universitarios')\\\n",
    "        .select(\"ID\",\"PD\")\\\n",
    "        .orderBy(desc(\"PD\")).head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2578faad-d963-4463-84ce-73e47e52582e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.\tGenere un cuadro consolidado donde indique la cantidad de personas que tienen estudios técnicos y estudios universitarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39120f36-2421-4a44-bea8-39acab4bc648",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_level_education = df_with_schema.filter(col(\"NIVEL_EDUCATIVO\").isin([\"Estudios Tecnicos\",\"Estudios Universitarios\"]))\\\n",
    "                     .groupBy(col(\"NIVEL_EDUCATIVO\")).count()\n",
    "df_level_education.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4ad1c55-69f6-48f6-a284-367624d612a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 6.\tDe la pregunta anterior, cree un dataframe solo con los de estudios técnicos y sus PD. Listelas de mayor a menor por PD. Seleccione un nuevo dataframe donde solo muestre los 5 registros con mejores PD y los 5 peores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0afcf39a-40f7-4788-bbb1-7261e406678e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_with_schema.display()\n",
    "from pyspark.sql.functions import desc\n",
    "df_filter_estTecnico=df_with_schema.select(\"ID\",\"PD\").filter(df_with_schema.NIVEL_EDUCATIVO==\"Estudios Tecnicos\").orderBy(desc(\"PD\"))\n",
    "\n",
    "\n",
    "df_filter_first=spark.createDataFrame(df_filter_estTecnico.head(5))\n",
    "df_filter_ult=spark.createDataFrame(df_filter_estTecnico.tail(5))\n",
    "\n",
    "df_union=df_filter_first.union(df_filter_ult)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08f34574-88d0-44a0-8fe0-f6e6d3985ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 7.\tSimilar a la pregunta 6, realice el mismo caso para los que tienen estudios universitarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c2adef3-8b5b-45a8-98c0-bcb68723c0fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filter_estUni=df_with_schema.select(\"ID\",\"PD\").filter(df_with_schema.NIVEL_EDUCATIVO==\"Estudios Universitarios\").orderBy(desc(\"PD\"))\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_estUni_first5=df_filter_estUni.limit(5)\n",
    "df_estUni_tail5=spark.createDataFrame(df_filter_estUni.tail(5))\n",
    "\n",
    "df=df_estUni_first5.union(df_estUni_tail5)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebebd772-8221-417c-bab5-4ba15dbff312",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 8.\tTomando la muestra de la ¿ pregunta 6 y 7. Quiénes tienen menor probabilidad de default? (PD). Nota: Si el valor es menor, tiene menor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "122b1990-d4ae-47f3-a0f3-d140fc2594eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_default = df_with_schema.select(col(\"DEFAULT\"))\n",
    "df_default.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8fc086e-fadb-4df2-b48b-b4a069bc9ebc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_default = df_default.withColumn(\"valor_default\", when(col(\"DEFAULT\") == \"0\", \"Menor Probabilidad\")\n",
    "                                      .when(col(\"DEFAULT\") == \"1\", \"Mayor probabilidad\")\n",
    "                                      .otherwise(col(\"DEFAULT\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f7519cd-81e7-411d-9fce-28dea38cd62a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 9.\tCree un Dataframe donde solo incluya los campos: ID, Nivel Educativo, Sexo, Edad, PD. Adicionalmente le solicitan estrezar el portafolio de clientes y su riesgo, por lo que a la variable Edad se le aplicará un factor en función del ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b914d1c4-1451-47a6-a274-e1e6c2214a17",
     "showTitle": false,
     "title": "a.\tSi el ID es 1, el factor es 0.0001, entonces se multiplicará la Edad + (Edad*0.0001)"
    }
   },
   "outputs": [],
   "source": [
    "df_stressed = df_with_schema.select(\"ID\",\"Nivel_Educativo\",\"Sexo\",\"Edad\",\"PD\")\n",
    "df_stressed.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3711fa11-187d-475f-a83e-57484eb81bd1",
     "showTitle": true,
     "title": "a.\tSi el ID es 1, el factor es 0.0001, entonces se multiplicará la Edad + (Edad*0.0001)"
    }
   },
   "outputs": [],
   "source": [
    "df_stressed = df_stressed.withColumn(\"Factor\",col(\"Edad\")*col(\"ID\")/10000)\n",
    "df_stressed.display(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98cefc23-ff17-4a99-ae3d-23e9129bbba3",
     "showTitle": true,
     "title": "b.\tA medida que el ID aumenta en 1, el factor aumenta en la misma proporción, es decir, para el ID 2, el factor es 0.0001, y así sucesivamente. "
    }
   },
   "outputs": [],
   "source": [
    "#SOLO ES ACLARACIÓN DE LA A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ab26f23-336f-41ab-823c-44aac12d825e",
     "showTitle": true,
     "title": "c.\tLa PD va a tener el mismo comportamiento. En el caso que la PD sobrepase el valor de 1, este ya no sufrirá variación."
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import asc\n",
    "df_stressed = df_stressed.withColumn(\"PDF\", F.when(df_stressed.PD > 0.01, col(\"PD\")).otherwise(col(\"PD\")*col(\"Edad\"))).orderBy(asc(\"PD\"))\n",
    "df_stressed.display(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3ff6f55-4a5b-4d87-8be8-e1bd4d260580",
     "showTitle": true,
     "title": "d.\tPresente el reporte con el cálculo ajustado."
    }
   },
   "outputs": [],
   "source": [
    "df_stressed_comparative = df_stressed.select(\"PD\",\"PDF\")\n",
    "df_stressed_comparative.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c424e60b-ef08-4f2e-b0e2-7871b42e9760",
     "showTitle": true,
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "df_stressed_comparative=df_stressed_comparative.withColumn(\"Resta_pd_pdf\",col(\"PD\")-col(\"PDF\"))\n",
    "print(df_stressed_comparative.filter(df_stressed_comparative.Resta_pd_pdf==0).count()/ df_stressed_comparative.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afb34384-eee7-4e25-95a9-423c0ba429cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### EL PDF (PD calculado) difiere del PD solo en un 1% del total de registros existentes. Por lo que podemos indicar que no existe mucha diferencia."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "S4_Databricks_JAAR",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
